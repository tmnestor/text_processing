{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polars Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Hashed Unique IDs:\n",
      "shape: (3, 3)\n",
      "┌─────────┬───────────────┬─────────────────────────────────┐\n",
      "│ Hash_Id ┆ Cleaned_Claim ┆ Hash_Id_hashed                  │\n",
      "│ ---     ┆ ---           ┆ ---                             │\n",
      "│ i64     ┆ str           ┆ str                             │\n",
      "╞═════════╪═══════════════╪═════════════════════════════════╡\n",
      "│ 1       ┆ A             ┆ 6b86b273ff34fce19d6b804eff5a3f… │\n",
      "│ 2       ┆ B             ┆ d4735e3a265e16eee03f59718b9b5d… │\n",
      "│ 3       ┆ C             ┆ 4e07408562bedb8b60ce05c1decfe3… │\n",
      "└─────────┴───────────────┴─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import hashlib\n",
    "\n",
    "# Function to convert text to SHA-256 hash\n",
    "def text_to_sha256(text):\n",
    "    # Ensure the input is converted to string and encoded to bytes\n",
    "    return hashlib.sha256(str(text).encode('utf-8')).hexdigest()\n",
    "\n",
    "# Initialize the DataFrame\n",
    "df = pl.DataFrame({\n",
    "    'Hash_Id': [1, 2, 3],\n",
    "    'Cleaned_Claim': ['A', 'B', 'C']\n",
    "})\n",
    "\n",
    "# Apply the hashing function to the 'Hash_Id' column\n",
    "df_hashed = df.with_columns(\n",
    "    df['Hash_Id'].map_elements(text_to_sha256, return_dtype=pl.Utf8).alias('Hash_Id_hashed')\n",
    ")\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(\"DataFrame with Hashed Unique IDs:\")\n",
    "print(df_hashed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame:\n",
      "shape: (6, 3)\n",
      "┌─────────┬───────────────┬─────────────┐\n",
      "│ Hash_Id ┆ Cleaned_Claim ┆ FTC_Label   │\n",
      "│ ---     ┆ ---           ┆ ---         │\n",
      "│ i64     ┆ str           ┆ str         │\n",
      "╞═════════╪═══════════════╪═════════════╡\n",
      "│ 1       ┆ Claim A       ┆ Label A     │\n",
      "│ 2       ┆ Claim B       ┆ Label B     │\n",
      "│ 3       ┆ Claim C       ┆ Label C     │\n",
      "│ 4       ┆ Claim D_new   ┆ Label D_new │\n",
      "│ 6       ┆ Claim F_new   ┆ Label F_new │\n",
      "│ 5       ┆ Claim E       ┆ Label E     │\n",
      "└─────────┴───────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Initialize sample DataFrames\n",
    "df = pl.DataFrame({\n",
    "    'Hash_Id': [1, 2, 3,5],\n",
    "    'Cleaned_Claim': ['Claim A', 'Claim B', 'Claim C', 'Claim E'],\n",
    "    'FTC_Label': ['Label A', 'Label B', 'Label C', 'Label E']\n",
    "})\n",
    "\n",
    "df_new = pl.DataFrame({\n",
    "    'Hash_Id': [1, 2, 3, 4,6],\n",
    "    'Cleaned_Claim': ['Claim A','Claim B_updated', 'Claim C_updated', 'Claim D_new', 'Claim F_new'],\n",
    "    'FTC_Label': ['Label A','Label B_updated', 'Label C_updated', 'Label D_new', 'Label F_new']\n",
    "})\n",
    "\n",
    "# Use 'full' join to merge DataFrames on 'Hash_Id'\n",
    "merged_df = df.join(df_new, on='Hash_Id', how='full', suffix='_new')\n",
    "\n",
    "# df = merged_df.with_columns(\n",
    "#     pl.when(pl.col(\"Hash_Id_new\").is_null())\n",
    "#     .then(pl.col(\"Hash_Id\"))\n",
    "#     .otherwise(pl.col(\"Hash_Id_new\"))\n",
    "#     .alias('Hash_Id'),\n",
    "\n",
    "#     pl.when(pl.col(\"Cleaned_Claim_new\").is_null())\n",
    "#     .then(pl.col(\"Cleaned_Claim\"))\n",
    "#     .otherwise(pl.col(\"Cleaned_Claim_new\"))\n",
    "#     .alias('Cleaned_Claim')\n",
    "# ).select(['Hash_Id', 'Cleaned_Claim'])\n",
    "\n",
    "result_df = merged_df.select([\n",
    "    pl.when(pl.col('Hash_Id').is_not_null())\n",
    "      .then(pl.col('Hash_Id'))\n",
    "      .otherwise(pl.col('Hash_Id_new'))\n",
    "      .alias('Hash_Id'),\n",
    "\n",
    "    pl.when(pl.col('Cleaned_Claim').is_not_null())\n",
    "      .then(pl.col('Cleaned_Claim'))\n",
    "      .otherwise(pl.col('Cleaned_Claim_new'))\n",
    "      .alias('Cleaned_Claim'),\n",
    "\n",
    "    pl.when(pl.col('FTC_Label').is_not_null())\n",
    "      .then(pl.col('FTC_Label'))\n",
    "      .otherwise(pl.col('FTC_Label_new'))\n",
    "      .alias('FTC_Label')\n",
    "])\n",
    "\n",
    "\n",
    "# Print the merged DataFrame\n",
    "print(\"Merged DataFrame:\")\n",
    "print(result_df)\n",
    "\n",
    "# updated_rows_filter = (merged_df['Cleaned_Claim'] != merged_df['Cleaned_Claim_new']) & merged_df['Cleaned_Claim_new'].is_not_null()\n",
    "# updated_rows = updated_rows_filter.sum()\n",
    "# print(\"Number of updated rows:\", updated_rows)\n",
    "# inserted_rows = df_new.select('Hash_Id').join(df, on='Hash_Id', how='anti').shape[0]\n",
    "# print(f\"Number of inserted rows: {inserted_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrames are equal up to row permutations: True\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Suppose df1 and df2 are your DataFrames\n",
    "df1 = pl.DataFrame({\n",
    "    \"A\": [1, 2, 3],\n",
    "    \"B\": [\"a\", \"b\", \"c\"]\n",
    "})\n",
    "\n",
    "df2 = pl.DataFrame({\n",
    "    \"A\": [3, 1, 2],\n",
    "    \"B\": [\"c\", \"a\", \"b\"]\n",
    "})\n",
    "\n",
    "# Sort both dataframes by all columns\n",
    "sorted_df1 = df1.sort(by=df1.columns)\n",
    "sorted_df2 = df2.sort(by=df2.columns)\n",
    "\n",
    "# Check if the sorted DataFrames are equal\n",
    "are_equal = sorted_df1.equals(sorted_df2)\n",
    "\n",
    "# Output the result\n",
    "print(\"The DataFrames are equal up to row permutations:\", are_equal)\n",
    "\n",
    "# pl.testing.assert_frame_equal(sorted_df1, sorted_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize the DataFrames\n",
    "df = pd.DataFrame({\n",
    "    'Hash_Id': [1, 2, 3],\n",
    "    'Cleaned_Claim': ['A', 'B', 'C']\n",
    "})\n",
    "\n",
    "df_new = pd.DataFrame({\n",
    "    'Hash_Id': [2, 3, 4],\n",
    "    'Cleaned_Claim': ['B_updated', 'C_updated', 'D']\n",
    "})\n",
    "\n",
    "# Set 'Hash_Id' as the index for both DataFrames\n",
    "df.set_index('Hash_Id', inplace=True)\n",
    "df_new.set_index('Hash_Id', inplace=True)\n",
    "\n",
    "# Copy the original DataFrame to use for the updated row count\n",
    "original_df = df.copy()\n",
    "\n",
    "# Update the DataFrame (UPSERT existing rows)\n",
    "df.update(df_new)\n",
    "\n",
    "# Identify updated rows by comparing the Cleaned_Claims\n",
    "updated_rows = df_new.index.intersection(original_df.index).size\n",
    "\n",
    "# Combine the DataFrames (this will add new rows)\n",
    "df_combined = df.combine_first(df_new)\n",
    "\n",
    "# Count the number of inserted rows by finding new indices\n",
    "inserted_rows = df_combined.index.difference(original_df.index).size\n",
    "\n",
    "# Reset index to make 'Hash_Id' a column again\n",
    "df_combined.reset_index(inplace=True)\n",
    "\n",
    "# Print results\n",
    "# print(f\"Updated rows: {updated_rows}\")\n",
    "print(f\"{df_new.index.intersection(original_df.index.to_list())=}\")\n",
    "print(f\"Inserted rows: {inserted_rows}\")\n",
    "print(\"Final DataFrame:\")\n",
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize the DataFrames\n",
    "df = pd.DataFrame({\n",
    "    'Hash_Id': [1, 2, 3],\n",
    "    'Cleaned_Claim': ['A', 'B', 'C']\n",
    "})\n",
    "\n",
    "df_new = pd.DataFrame({\n",
    "    'Hash_Id': [2, 3, 4],\n",
    "    'Cleaned_Claim': ['B_updated', 'C_updated', 'D']\n",
    "})\n",
    "\n",
    "# Set 'Hash_Id' as the index for both DataFrames\n",
    "df.set_index('Hash_Id', inplace=True)\n",
    "df_new.set_index('Hash_Id', inplace=True)\n",
    "\n",
    "# Copy the original DataFrame to use for the updated row count\n",
    "original_df = df.copy()\n",
    "\n",
    "# Update the DataFrame (UPSERT existing rows)\n",
    "df.update(df_new)\n",
    "\n",
    "# Identify updated rows by comparing the Cleaned_Claims\n",
    "# updated_rows = (df.loc[df_new.index.intersection(original_df.index)] != original_df).any(axis=1).sum()\n",
    "\n",
    "# Combine the DataFrames (this will add new rows)\n",
    "df_combined = df.combine_first(df_new)\n",
    "\n",
    "# Count the number of inserted rows by finding new indices\n",
    "inserted_rows = df_combined.index.difference(original_df.index).size\n",
    "\n",
    "# Reset index to make 'Hash_Id' a column again\n",
    "df_combined.reset_index(inplace=True)\n",
    "\n",
    "# Print results\n",
    "# print(f\"Updated rows: {updated_rows}\")\n",
    "print(f\"Inserted rows: {inserted_rows}\")\n",
    "print(\"Final DataFrame:\")\n",
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize sample DataFrames\n",
    "df = pd.DataFrame({\n",
    "    'Hash_Id': [1, 2, 3],\n",
    "    'Cleaned_Claim': ['A', 'B', 'C']\n",
    "})\n",
    "\n",
    "df_new = pd.DataFrame({\n",
    "    'Hash_Id': [2, 3, 4],\n",
    "    'Cleaned_Claim': ['B_updated', 'C_updated', 'D']\n",
    "})\n",
    "\n",
    "# Perform UPSERT operation\n",
    "df.set_index('Hash_Id', inplace=True)\n",
    "df_new.set_index('Hash_Id', inplace=True)\n",
    "\n",
    "# Keep a copy of the original DataFrame for comparison\n",
    "original_df = df.copy()\n",
    "\n",
    "# Update existing rows\n",
    "df.update(df_new)\n",
    "\n",
    "# Count the updated rows\n",
    "updated_rows = (df != original_df).any(axis=1).sum()\n",
    "\n",
    "# Append new rows\n",
    "df = df.combine_first(df_new)\n",
    "\n",
    "# Reset index to ensure Hash_Id is a column\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# Print the number of updated rows\n",
    "print(\"Number of updated rows:\", updated_rows)\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(\"Upserted DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Initialize sample DataFrames\n",
    "df = pl.DataFrame({\n",
    "    'Hash_Id': [1, 2, 3],\n",
    "    'Cleaned_Claim': ['A', 'B', 'C']\n",
    "})\n",
    "\n",
    "df_new = pl.DataFrame({\n",
    "    'Hash_Id': [2, 3, 4],\n",
    "    'Cleaned_Claim': ['B_updated', 'C_updated', 'D']\n",
    "})\n",
    "\n",
    "# Perform a full join on 'Hash_Id' to identify and update existing rows\n",
    "joined_df = df.join(df_new, on='Hash_Id', how='full', suffix='_new')\n",
    "\n",
    "# Determine updated rows where the 'Cleaned_Claim' in df is different from 'df_new'\n",
    "updated_rows_filter = (joined_df['Cleaned_Claim'] != joined_df['Cleaned_Claim_new']) & joined_df['Cleaned_Claim_new'].is_not_null()\n",
    "updated_rows = updated_rows_filter.sum()\n",
    "\n",
    "# Update Cleaned_Claims where available\n",
    "df_updated = joined_df.with_columns([\n",
    "    pl.when(joined_df['Cleaned_Claim_new'].is_null())\n",
    "    .then(joined_df['Cleaned_Claim'])\n",
    "    .otherwise(joined_df['Cleaned_Claim_new'])\n",
    "    .alias('final_Cleaned_Claim')\n",
    "])\n",
    "\n",
    "# Select columns and resolve renaming\n",
    "df_upserted = df_updated.select(['Hash_Id', 'final_Cleaned_Claim']).rename({'final_Cleaned_Claim': 'Cleaned_Claim'})\n",
    "\n",
    "# Count the number of inserted rows\n",
    "# Inserted rows are determined by rows in df_new not appearing in the original df\n",
    "inserted_rows = df_new.select('Hash_Id').join(df, on='Hash_Id', how='anti').shape[0]\n",
    "\n",
    "# Print the counts and the final DataFrame\n",
    "print(f\"Number of updated rows: {updated_rows}\")\n",
    "print(f\"Number of inserted rows: {inserted_rows}\")\n",
    "print(\"Upserted DataFrame:\")\n",
    "print(df_upserted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Initialize sample DataFrames\n",
    "df = pl.DataFrame({\n",
    "    'Hash_Id': [1, 2, 3],\n",
    "    'Cleaned_Claim': ['A', 'B', 'C']\n",
    "})\n",
    "\n",
    "df_new = pl.DataFrame({\n",
    "    'Hash_Id': [2, 3, 4],\n",
    "    'Cleaned_Claim': ['B_updated', 'C_updated', 'D']\n",
    "})\n",
    "\n",
    "# Perform a full join on 'Hash_Id' to identify and update existing rows\n",
    "joined_df = df.join(df_new, on='Hash_Id', how='full', suffix='_new')\n",
    "\n",
    "# Update Cleaned_Claims where available\n",
    "df_updated = joined_df.with_columns([\n",
    "    pl.when(joined_df['Cleaned_Claim_new'].is_null())\n",
    "    .then(joined_df['Cleaned_Claim'])\n",
    "    .otherwise(joined_df['Cleaned_Claim_new'])\n",
    "    .alias('final_Cleaned_Claim')\n",
    "])\n",
    "\n",
    "# Select the resultant columns and correct the index\n",
    "df_upserted = df_updated.select(['Hash_Id', 'final_Cleaned_Claim']).rename({'final_Cleaned_Claim': 'Cleaned_Claim'})\n",
    "\n",
    "# Set 'Hash_Id' as a column instead of allowing it to become an index by avoiding any drop errors\n",
    "df_upserted = df_upserted.filter(df_upserted['Hash_Id'].is_not_null())\n",
    "\n",
    "# Determine updated rows where the 'Cleaned_Claim' in df is different from 'df_new'\n",
    "updated_rows_filter = (joined_df['Cleaned_Claim'] != joined_df['Cleaned_Claim_new']) & joined_df['Cleaned_Claim_new'].is_not_null()\n",
    "updated_rows = updated_rows_filter.sum()\n",
    "\n",
    "# Count the number of inserted rows\n",
    "inserted_rows = df_new.select('Hash_Id').join(df, on='Hash_Id', how='anti').shape[0]\n",
    "\n",
    "# Print the counts and the final DataFrame\n",
    "print(f\"Number of updated rows: {updated_rows}\")\n",
    "print(f\"Number of inserted rows: {inserted_rows}\")\n",
    "print(\"Upserted DataFrame:\")\n",
    "print(df_upserted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Initialize sample DataFrames\n",
    "df = pl.DataFrame({\n",
    "    'Hash_Id': [1, 2, 3],\n",
    "    'Cleaned_Claim': ['A', 'B', 'C']\n",
    "})\n",
    "\n",
    "df_new = pl.DataFrame({\n",
    "    'Hash_Id': [2, 3, 4],\n",
    "    'Cleaned_Claim': ['B_updated', 'C_updated', 'D']\n",
    "})\n",
    "\n",
    "# Perform upsert operation in a more concise way\n",
    "df_upserted = (\n",
    "    df.join(\n",
    "        df_new,\n",
    "        on='Hash_Id',\n",
    "        how='outer'\n",
    "    ).with_columns(\n",
    "        pl.coalesce('Cleaned_Claim_right', 'Cleaned_Claim').alias('Cleaned_Claim')\n",
    "    ).select(['Hash_Id', 'Cleaned_Claim'])\n",
    ")\n",
    "\n",
    "# Calculate metrics\n",
    "updated_rows = (\n",
    "    df.join(df_new, on='Hash_Id')\n",
    "    .filter(pl.col('Cleaned_Claim') != pl.col('Cleaned_Claim_right'))\n",
    "    .shape[0]\n",
    ")\n",
    "\n",
    "inserted_rows = df_new.join(df, on='Hash_Id', how='anti').shape[0]\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of updated rows: {updated_rows}\")\n",
    "print(f\"Number of inserted rows: {inserted_rows}\")\n",
    "print(\"Upserted DataFrame:\")\n",
    "print(df_upserted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Initialize sample DataFrames\n",
    "df = pl.DataFrame({\n",
    "    'Hash_Id': [1, 2, 3],\n",
    "    'Cleaned_Claim': ['A', 'B', 'C']\n",
    "})\n",
    "\n",
    "df_new = pl.DataFrame({\n",
    "    'Hash_Id': [2, 3, 4],\n",
    "    'Cleaned_Claim': ['B_updated', 'C_updated', 'D']\n",
    "})\n",
    "\n",
    "# Perform upsert operation using how='full'\n",
    "df_upserted = (\n",
    "    df.join(\n",
    "        df_new,\n",
    "        on='Hash_Id',\n",
    "        how='full'\n",
    "    ).with_columns(\n",
    "        pl.coalesce('Cleaned_Claim_right', 'Cleaned_Claim').alias('Cleaned_Claim')\n",
    "    ).select(['Hash_Id', 'Cleaned_Claim'])\n",
    ")\n",
    "\n",
    "# Calculate metrics\n",
    "updated_rows = (\n",
    "    df.join(df_new, on='Hash_Id')\n",
    "    .filter(pl.col('Cleaned_Claim') != pl.col('Cleaned_Claim_right'))\n",
    "    .shape[0]\n",
    ")\n",
    "\n",
    "inserted_rows = df_new.join(df, on='Hash_Id', how='anti').shape[0]\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of updated rows: {updated_rows}\")\n",
    "print(f\"Number of inserted rows: {inserted_rows}\")\n",
    "print(\"Upserted DataFrame:\")\n",
    "print(df_upserted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Initialize sample DataFrames\n",
    "df = pl.DataFrame({\n",
    "    'Hash_Id': [1, 2, 3],\n",
    "    'Cleaned_Claim': ['A', 'B', 'C']\n",
    "})\n",
    "\n",
    "df_new = pl.DataFrame({\n",
    "    'Hash_Id': [2, 3, 4],\n",
    "    'Cleaned_Claim': ['B_updated', 'C_updated', 'D']\n",
    "})\n",
    "\n",
    "# Perform upsert operation with proper indexing\n",
    "df_upserted = (\n",
    "    df.join(\n",
    "        df_new,\n",
    "        on='Hash_Id',\n",
    "        how='full'\n",
    "    ).with_columns(\n",
    "        pl.coalesce('Cleaned_Claim_right', 'Cleaned_Claim').alias('Cleaned_Claim')\n",
    "    ).select(['Hash_Id', 'Cleaned_Claim'])\n",
    "    .sort('Hash_Id')  # Add sorting to maintain index order\n",
    ")\n",
    "\n",
    "# Calculate metrics\n",
    "updated_rows = (\n",
    "    df.join(df_new, on='Hash_Id')\n",
    "    .filter(pl.col('Cleaned_Claim') != pl.col('Cleaned_Claim_right'))\n",
    "    .shape[0]\n",
    ")\n",
    "\n",
    "inserted_rows = df_new.join(df, on='Hash_Id', how='anti').shape[0]\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of updated rows: {updated_rows}\")\n",
    "print(f\"Number of inserted rows: {inserted_rows}\")\n",
    "print(\"Upserted DataFrame:\")\n",
    "print(df_upserted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Upsert and Merge with Delta Lake Tables in Python Polars](https://stuffbyyuki.com/upsert-and-merge-with-delta-lake-tables-in-python-polars/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.DataFrame(\n",
    "    {\n",
    "        'key': [1, 2, 3],\n",
    "        'letter': ['a', 'b', 'c'],\n",
    "        'Cleaned_Claim': [100, 200, 300]\n",
    "    }\n",
    ")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_table_path = './my_delta_lake_table'\n",
    "\n",
    "df.write_delta(output_table_path, mode='overwrite')\n",
    "print('The target Delta Lake table output:\\n', pl.read_delta(output_table_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col_updated_and_row_deleted = (\n",
    "    df\n",
    "    .with_columns(\n",
    "        pl.when(pl.col('letter')=='c')  # update a column \n",
    "        .then(pl.lit('d'))\n",
    "        .otherwise(pl.col('letter'))  \n",
    "        .alias('letter')\n",
    "    )\n",
    "    .filter(pl.col('key') != 1)  # delete a row \n",
    ")\n",
    "\n",
    "df_with_changes = (\n",
    "    pl.concat(\n",
    "        [\n",
    "            df_col_updated_and_row_deleted,\n",
    "            pl.DataFrame({'key': 4, 'letter': 'd', 'Cleaned_Claim': 400})  # a new row\n",
    "        ],\n",
    "        how='vertical'\n",
    "    )\n",
    ")\n",
    "print(df_with_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_with_changes\n",
    "    .write_delta(\n",
    "        output_table_path,\n",
    "        mode='merge',\n",
    "        delta_merge_options={\n",
    "            'predicate': 'source.key = target.key',\n",
    "            'source_alias': 'source',\n",
    "            'target_alias': 'target',\n",
    "        },\n",
    "    )\n",
    "    .when_matched_update_all()\n",
    "    .when_not_matched_insert_all()\n",
    "    .when_not_matched_by_source_delete()\n",
    "    .execute()\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE VOLATILE TABLE result_table, MULTISET AS (\n",
    "    SELECT customer_id, \n",
    "           SUM(order_amount) as total_spent\n",
    "    FROM orders \n",
    "    WHERE order_date >= DATE '2024-01-01'\n",
    "    GROUP BY customer_id\n",
    ") WITH DATA\n",
    "PRIMARY INDEX (customer_id)\n",
    "ON COMMIT PRESERVE ROWS;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT TO_CHAR(timestamp_column, 'YYYY-MM-DD HH:MI:SS') FROM table_name;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Fastload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teradatasql\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Hash_Id': ['value1', 'value2', 'value3'],\n",
    "    'Clean_Claim': ['text1', 'text2', 'text3'],\n",
    "    'FTC_Label': ['data1', 'data2', 'data3']\n",
    "})\n",
    "\n",
    "# Connection parameters\n",
    "conn_params = {\n",
    "    'host': 'your_host',\n",
    "    'user': 'your_user',\n",
    "    'password': 'your_password'\n",
    "}\n",
    "\n",
    "create_tables_sql = \"\"\"\n",
    "CREATE MULTISET TABLE table_name (\n",
    "    Hash_Id CHAR(128),\n",
    "    Clean_Claim VARCHAR(80),\n",
    "    FTC_Label VARCHAR(20)\n",
    ") PRIMARY INDEX (Hash_Id);\n",
    "\n",
    "CREATE MULTISET TABLE error_table_1 (\n",
    "    Hash_Id CHAR(128),\n",
    "    Clean_Claim VARCHAR(80),\n",
    "    FTC_Label VARCHAR(20),\n",
    "    error_code INTEGER,\n",
    "    error_desc VARCHAR(256)\n",
    ") PRIMARY INDEX (Hash_Id);\n",
    "\n",
    "CREATE MULTISET TABLE error_table_2 (\n",
    "    Hash_Id CHAR(128),\n",
    "    Clean_Claim VARCHAR(80),\n",
    "    FTC_Label VARCHAR(20),\n",
    "    error_code INTEGER,\n",
    "    error_desc VARCHAR(256)\n",
    ") PRIMARY INDEX (Hash_Id);\n",
    "\"\"\"\n",
    "\n",
    "with teradatasql.connect(**conn_params) as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        # Create tables\n",
    "        for stmt in create_tables_sql.split(';'):\n",
    "            if stmt.strip():\n",
    "                cur.execute(stmt)\n",
    "        \n",
    "        # FastLoad setup\n",
    "        cur.execute(\"SESSIONS 8\")\n",
    "        cur.execute(\"ERRLIMIT 50\")\n",
    "        cur.execute(\"\"\"\n",
    "            BEGIN LOADING table_name\n",
    "            ERRORFILES error_table_1, error_table_2;\n",
    "        \"\"\")\n",
    "        cur.execute(\"SET RECORD VARTEXT ','\")\n",
    "        cur.execute(\"\"\"\n",
    "            DEFINE Hash_Id (CHAR(128)),\n",
    "                   Clean_Claim (VARCHAR(80)),\n",
    "                   FTC_Label (VARCHAR(20))\n",
    "        \"\"\")\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO table_name (Hash_Id, Clean_Claim, FTC_Label)\n",
    "            VALUES (:Hash_Id, :Clean_Claim, :FTC_Label)\n",
    "        \"\"\")\n",
    "        \n",
    "        # Load data from DataFrame using plain tuples\n",
    "        for row in df.itertuples(index=False, name=None):\n",
    "            cur.execute(\"INSERT INTO table_name VALUES (?, ?, ?)\", row)\n",
    "        \n",
    "        # End loading\n",
    "        cur.execute(\"END LOADING\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polars FastLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teradatasql\n",
    "import polars as pl\n",
    "\n",
    "# Create sample DataFrame\n",
    "df = pl.DataFrame({\n",
    "    'Hash_Id': ['value1', 'value2', 'value3'],\n",
    "    'Clean_Claim': ['text1', 'text2', 'text3'],\n",
    "    'FTC_Label': ['data1', 'data2', 'data3']\n",
    "})\n",
    "\n",
    "# Connection parameters\n",
    "conn_params = {\n",
    "    'host': 'your_host',\n",
    "    'user': 'your_user',\n",
    "    'password': 'your_password'\n",
    "}\n",
    "\n",
    "create_tables_sql = \"\"\"\n",
    "CREATE MULTISET TABLE table_name (\n",
    "    Hash_Id CHAR(128),\n",
    "    Clean_Claim VARCHAR(80),\n",
    "    FTC_Label VARCHAR(20)\n",
    ") PRIMARY INDEX (Hash_Id);\n",
    "\n",
    "CREATE MULTISET TABLE error_table_1 (\n",
    "    Hash_Id CHAR(128),\n",
    "    Clean_Claim VARCHAR(80),\n",
    "    FTC_Label VARCHAR(20),\n",
    "    error_code INTEGER,\n",
    "    error_desc VARCHAR(256)\n",
    ") PRIMARY INDEX (Hash_Id);\n",
    "\n",
    "CREATE MULTISET TABLE error_table_2 (\n",
    "    Hash_Id CHAR(128),\n",
    "    Clean_Claim VARCHAR(80),\n",
    "    FTC_Label VARCHAR(20),\n",
    "    error_code INTEGER,\n",
    "    error_desc VARCHAR(256)\n",
    ") PRIMARY INDEX (Hash_Id);\n",
    "\"\"\"\n",
    "\n",
    "with teradatasql.connect(**conn_params) as conn:\n",
    "    with conn.cursor() as cur:\n",
    "        # Create tables\n",
    "        for stmt in create_tables_sql.split(';'):\n",
    "            if stmt.strip():\n",
    "                cur.execute(stmt)\n",
    "        \n",
    "        # FastLoad setup\n",
    "        cur.execute(\"SESSIONS 8\")\n",
    "        cur.execute(\"ERRLIMIT 50\")\n",
    "        cur.execute(\"\"\"\n",
    "            BEGIN LOADING table_name\n",
    "            ERRORFILES error_table_1, error_table_2;\n",
    "        \"\"\")\n",
    "        cur.execute(\"SET RECORD VARTEXT ','\")\n",
    "        cur.execute(\"\"\"\n",
    "            DEFINE Hash_Id (CHAR(128)),\n",
    "                   Clean_Claim (VARCHAR(80)),\n",
    "                   FTC_Label (VARCHAR(20))\n",
    "        \"\"\")\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO table_name (Hash_Id, Clean_Claim, FTC_Label)\n",
    "            VALUES (:Hash_Id, :Clean_Claim, :FTC_Label)\n",
    "        \"\"\")\n",
    "        \n",
    "        # Load data from DataFrame using Polars iteration\n",
    "        for row in df.iter_rows():\n",
    "            cur.execute(\"INSERT INTO table_name VALUES (?, ?, ?)\", row)\n",
    "        \n",
    "        # End loading\n",
    "        cur.execute(\"END LOADING\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "setfit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
